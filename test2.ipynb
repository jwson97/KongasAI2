{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading document: docs/가스계통_운영규정.pdf\n",
      "Loading document: docs/여비규정.pdf\n",
      "Loading document: docs/취업규칙.pdf\n",
      "제29조(연차유급휴가)와 관련된 규정을 살펴보면, 연차휴가를 다 사용하지 못한 경우에 대한 규정이 있습니다.\n",
      "\n",
      "1. 제29조 제5항에 따르면, 연차유급휴가 중 12일의 범위 내에서 사용촉진조치를 시행하며, 그 미사용 휴가는 연간 10일 한도로 5년간 저축하여 사용할 수 있습니다. 그러나 5년 이내 또는 퇴직 시까지 사용하지 않은 저축 연차휴가는 자동소멸됩니다.\n",
      "\n",
      "2. 제29조 제6항에 따르면, 회사의 형편에 따라 연차휴가를 사용할 수 없거나 적치하지 않은 경우에는 보수규정이 정하는 바에 따라 수당을 지급할 수 있습니다.\n",
      "\n",
      "따라서, 연차휴가를 다 사용하지 못한 경우, 일부는 저축하여 사용할 수 있으며, 회사의 사정에 따라 수당으로 받을 수도 있습니다.\n",
      "\n",
      "Source Regulations:\n",
      "- [Document Name] 제29조(연차유급휴가) 제5항: \"연차휴가는 직원의 자유의사에 따라 적치하여 계산기간 만료 익일부터 1년 이내에 사용한다. 단, 공사가 제29조 제1항, 제3항 및 제4항에 따른 연차유급휴가 중 12일의 범위 내에 근로기준법 제61조에 따라 사용촉진조치를 시행하며 그 미사용 휴가는 연간 10일 한도로 5년간 저축 사용 할 수 있으나 5년 이내 또는 퇴직 시까지 사용하지 아니한 저축 연차휴가는 자동소멸된다.\"\n",
      "- [Document Name] 제29조(연차유급휴가) 제6항: \"공사의 형편에 따라 연차휴가를 사용할 수 없거나 적치하지 아니한 때에는 보수규정이 정하는 바에 따라 수당을 지급한다.\"\n",
      "\n",
      "Detailed Source Information:\n",
      "\n",
      "Final Messages:\n",
      "user: Hi...\n",
      "assistant: Hi there! How can I assist you today?...\n",
      "user: 일이 너무 많아서 연차휴가를 할당된 만큼 다 쓰지 못할거 같아. 그럼 남는 연차휴가는 어떻...\n",
      "assistant: *(RAG Response)*\n",
      "제29조(연차유급휴가)와 관련된 규정을 살펴보면, 연차휴가를...\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.schema import Document\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# Define document paths\n",
    "doc_paths = [\n",
    "    \"docs/가스계통_운영규정.pdf\",\n",
    "    \"docs/여비규정.pdf\",\n",
    "    \"docs/취업규칙.pdf\",\n",
    "]\n",
    "\n",
    "# Load documents\n",
    "docs = []\n",
    "for doc_file in doc_paths:\n",
    "    file_path = Path(doc_file)\n",
    "    print(\"Loading document:\", doc_file)\n",
    "    try:\n",
    "        if doc_file.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            for page in loader.load_and_split():\n",
    "                # Create a new Document with page number in metadata\n",
    "                doc = Document(\n",
    "                    page_content=page.page_content,\n",
    "                    metadata={\n",
    "                        \"source\": doc_file,\n",
    "                        \"page\": page.metadata[\"page\"]\n",
    "                    }\n",
    "                )\n",
    "                docs.append(doc)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading document {doc_file}: {e}\")\n",
    "\n",
    "# Split documents\n",
    "text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "document_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create vector store\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=document_chunks,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "# Create retrievers\n",
    "bm25_retriever = BM25Retriever.from_documents(document_chunks)\n",
    "bm25_retriever.k = 3\n",
    "\n",
    "chroma_retriever = vector_db.as_retriever(search_kwargs={'k': 3})\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[chroma_retriever, bm25_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")\n",
    "\n",
    "# Create LLM\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Create MultiQueryRetriever\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=ensemble_retriever,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "def _get_context_retriever_chain(vector_db, ensemble_retriever, llm):\n",
    "    multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "        retriever=ensemble_retriever,\n",
    "        llm=llm\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation, focusing on the most recent messages.\"),\n",
    "    ])\n",
    "    retriever_chain = create_history_aware_retriever(llm, ensemble_retriever, prompt)\n",
    "\n",
    "    return retriever_chain\n",
    "\n",
    "def get_conversational_rag_chain(llm, retriever):\n",
    "    retriever_chain = _get_context_retriever_chain(vector_db, retriever, llm)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "        \"\"\"You are an assistant designed specifically for answering queries based on company regulations. Always respond strictly according to the company's internal regulations, ensuring your answers are aligned with these rules. \n",
    "        When providing an answer, first cite the most relevant regulation in detail, including chapter and section numbers if applicable. If multiple regulations apply, list all relevant ones before giving your response. \n",
    "        Your goal is to provide the user with clear guidance based on the regulations, so be as specific as possible with the details of the rules and regulations before proceeding with the final answer.\n",
    "        If no regulation directly applies, inform the user and give your best guidance based on your knowledge of the company's practices.\n",
    "        \n",
    "        After your explanation, provide the exact quotes from the relevant regulations under a \"Source Regulations:\" section. Format each quote as follows:\n",
    "        [Document Name] Chapter X, Section Y (Page Z): \"Exact quote from the regulation\"\n",
    "\n",
    "        {context}\"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ])\n",
    "    stuff_documents_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "    return create_retrieval_chain(retriever_chain, stuff_documents_chain)\n",
    "\n",
    "# Create streaming LLM\n",
    "llm_stream_openai = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "# Define messages\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi there! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"일이 너무 많아서 연차휴가를 할당된 만큼 다 쓰지 못할거 같아. 그럼 남는 연차휴가는 어떻게 되지? 남는 연차 휴가에 대해 돈으로 받을수 있어?\"},\n",
    "]\n",
    "\n",
    "# Convert dict messages to HumanMessage and AIMessage objects\n",
    "langchain_messages = [HumanMessage(content=m[\"content\"]) if m[\"role\"] == \"user\" else AIMessage(content=m[\"content\"]) for m in messages]\n",
    "\n",
    "# Create conversational RAG chain\n",
    "conversation_rag_chain = get_conversational_rag_chain(llm_stream_openai, multi_query_retriever)\n",
    "\n",
    "# Process response\n",
    "def process_response(response):\n",
    "    source_regex = r\"Source Regulations:(.*?)(?=\\n\\n|\\Z)\"\n",
    "    source_match = re.search(source_regex, response, re.DOTALL)\n",
    "    \n",
    "    if source_match:\n",
    "        sources = source_match.group(1).strip().split('\\n')\n",
    "        for source in sources:\n",
    "            match = re.match(r\"\\[(.*?)\\](.*?)\\(Page (\\d+)\\): \\\"(.*)\\\"\", source)\n",
    "            if match:\n",
    "                doc_name, section, page, quote = match.groups()\n",
    "                print(f\"\\nReference from {doc_name}:\")\n",
    "                print(f\"Section: {section.strip()}\")\n",
    "                print(f\"Page: {page}\")\n",
    "                print(f\"Quote: {quote}\")\n",
    "                print(\"------------------------\")\n",
    "\n",
    "# Generate and process response\n",
    "response_message = \"*(RAG Response)*\\n\"\n",
    "last_user_message = messages[-1][\"content\"]\n",
    "\n",
    "for chunk in conversation_rag_chain.pick(\"answer\").stream({\"messages\": langchain_messages[:-1], \"input\": last_user_message}):\n",
    "    response_message += chunk\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\nDetailed Source Information:\")\n",
    "process_response(response_message)\n",
    "\n",
    "# Append the assistant's response to the messages list\n",
    "messages.append({\"role\": \"assistant\", \"content\": response_message})\n",
    "\n",
    "# Print final messages for verification\n",
    "print(\"\\nFinal Messages:\")\n",
    "for msg in messages:\n",
    "    print(f\"{msg['role']}: {msg['content'][:50]}...\")  # Print first 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kongasAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
