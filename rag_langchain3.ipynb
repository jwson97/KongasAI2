{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_file docs/가스계통_운영규정.pdf\n",
      "doc_file docs/여비규정.pdf\n",
      "doc_file docs/취업규칙.pdf\n"
     ]
    }
   ],
   "source": [
    "# Build a sample vectorDB\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, KonlpyTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.retrievers  import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Load blog post\n",
    "# Load docs\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # 이 함수를 호출하면 .env 파일의 내용이 환경 변수로 로드됩니다\n",
    "\n",
    "doc_paths = [\n",
    "    \"docs/가스계통_운영규정.pdf\",\n",
    "    \"docs/여비규정.pdf\",\n",
    "    \"docs/취업규칙.pdf\",\n",
    "]\n",
    "\n",
    "docs = [] \n",
    "for doc_file in doc_paths:\n",
    "    file_path = Path(doc_file)\n",
    "\n",
    "    print(\"doc_file\", doc_file)\n",
    "    try:\n",
    "        if doc_file.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "\n",
    "        docs.extend(loader.load())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading document {doc_file.name}: {e}\")\n",
    "\n",
    "\n",
    "# Split docs\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    # chunk_size=1000,\n",
    "    # chunk_overlap=300,\n",
    "    OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "document_chunks = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and load the documents to the vector store\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=document_chunks,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(document_chunks)\n",
    "\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "# Create MultiQueryRetriever\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vector_db.as_retriever(search_kwargs={'k': 5}),\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "\n",
    "def _get_context_retriever_chain(vector_db, llm):\n",
    "    chroma_retriever = vector_db.as_retriever(search_kwargs={'k':5})\n",
    "\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        # retrievers=[bm25_retriever, chroma_retriever, multi_query_retriever],\n",
    "        # weights=[0.2, 0.4, 0.4]\n",
    "\n",
    "        retrievers=[chroma_retriever, bm25_retriever],\n",
    "        weights=[0.5, 0.5]        \n",
    "    )\n",
    "\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\"user\", \"Given the above conversation, generate a search query to look up in order to get inforamtion relevant to the conversation, focusing on the most recent messages.\"),\n",
    "    ])\n",
    "    retriever_chain = create_history_aware_retriever(llm, ensemble_retriever, prompt)\n",
    "\n",
    "    return retriever_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversational_rag_chain(llm):\n",
    "    retriever_chain = _get_context_retriever_chain(vector_db, llm)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "        \"\"\"You are an assistant designed specifically for answering queries based on company regulations. Always respond strictly according to the company's internal regulations, ensuring your answers are aligned with these rules. \n",
    "        When providing an answer, first cite the most relevant regulation in detail, including chapter and section numbers if applicable. If multiple regulations apply, list all relevant ones before giving your response. \n",
    "        Your goal is to provide the user with clear guidance based on the regulations, so be as specific as possible with the details of the rules and regulations before proceeding with the final answer.\n",
    "        If no regulation directly applies, inform the user and give your best guidance based on your knowledge of the company's practices.\\n\n",
    "        {context}\"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ])\n",
    "    stuff_documents_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "    return create_retrieval_chain(retriever_chain, stuff_documents_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented Generation\n",
    "\n",
    "llm_stream_openai = ChatOpenAI(\n",
    "    model=\"gpt-4o\",  # Here you could use \"o1-preview\" or \"o1-mini\" if you already have access to them\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제29조(연차유급휴가)에 따르면, 연차휴가는 직원의 자유의사에 따라 적치하여 계산기간 만료 익일부터 1년 이내에 사용해야 합니다. 또한, 공사가 제29조 제1항, 제3항 및 제4항에 따른 연차유급휴가 중 12일의 범위 내에 근로기준법 제61조에 따라 사용촉진조치를 시행하며 그 미사용 휴가는 연간 10일 한도로 5년간 저축 사용 할 수 있으나 5년 이내 또는 퇴직 시까지 사용하지 아니한 저축 연차휴가는 자동소멸됩니다. \n",
      "\n",
      "제29조 제6항에 따르면, 공사의 형편에 따라 연차휴가를 사용할 수 없거나 적치하지 아니한 때에는 보수규정이 정하는 바에 따라 수당을 지급한다고 명시되어 있습니다.\n",
      "\n",
      "따라서, 연차휴가를 다 사용하지 못할 경우, 회사의 형편에 따라 수당으로 받을 수 있는 가능성이 있습니다. 다만, 이는 보수규정에 따라 결정되므로 구체적인 사항은 해당 규정을 참고하시기 바랍니다."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "llm_stream = llm_stream_openai  # Select between OpenAI and Anthropic models for the response\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi there! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"일이 너무 많아서 연차휴가를 할당된 만큼 다 쓰지 못할거 같아. 그럼 남는 연차휴가는 어떻게 되지? 남는 연차 휴가에 대해 돈으로 받을수 있어?\"},\n",
    "]\n",
    "messages = [HumanMessage(content=m[\"content\"]) if m[\"role\"] == \"user\" else AIMessage(content=m[\"content\"]) for m in messages]\n",
    "\n",
    "conversation_rag_chain = get_conversational_rag_chain(llm_stream)\n",
    "response_message = \"*(RAG Response)*\\n\"\n",
    "for chunk in conversation_rag_chain.pick(\"answer\").stream({\"messages\": messages[:-1], \"input\": messages[-1].content}):\n",
    "    response_message += chunk\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response_message})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_file docs/가스계통_운영규정.pdf\n",
      "doc_file docs/여비규정.pdf\n",
      "doc_file docs/취업규칙.pdf\n",
      "회사 규정에 따르면, 연차휴가는 직원의 자유의사에 따라 적치하여 계산기간 만료 익일부터 1년 이내에 사용해야 합니다. 또한, 회사는 제29조 제1항, 제3항 및 제4항에 따른 연차유급휴가 중 12일의 범위 내에 근로기준법 제61조에 따라 사용촉진조치를 시행하며 그 미사용 휴가는 연간 10일 한도로 5년간 저축 사용 할 수 있습니다. 하지만, 5년 이내 또는 퇴직 시까지 사용하지 아니한 저축 연차휴가는 자동소멸됩니다.\n",
      "\n",
      "또한, 공사의 형편에 따라 연차휴가를 사용할 수 없거나 적치하지 아니한 때에는 보수규정이 정하는 바에 따라 수당을 지급한다는 규정이 있습니다. 이는 특정 상황에서만 적용되며, 일반적으로 연차휴가를 돈으로 환산하여 받는 것은 허용되지 않습니다.\n",
      "\n",
      "Source Regulations:\n",
      "[근"
     ]
    }
   ],
   "source": [
    "# Build a sample vectorDB\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, KonlpyTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader, Docx2txtLoader, TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.retrievers  import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Load docs\n",
    "\n",
    "doc_paths = [\n",
    "    \"docs/가스계통_운영규정.pdf\",\n",
    "    \"docs/여비규정.pdf\",\n",
    "    \"docs/취업규칙.pdf\",\n",
    "]\n",
    "\n",
    "docs = [] \n",
    "for doc_file in doc_paths:\n",
    "    file_path = Path(doc_file)\n",
    "\n",
    "    print(\"doc_file\", doc_file)\n",
    "    try:\n",
    "        if doc_file.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "\n",
    "        docs.extend(loader.load())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading document {doc_file.name}: {e}\")\n",
    "\n",
    "# Split docs\n",
    "\n",
    "text_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "document_chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Tokenize and load the documents to the vector store\n",
    "\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=document_chunks,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "\n",
    "# BM25Retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(document_chunks)\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "# Chroma Retriever\n",
    "chroma_retriever = vector_db.as_retriever(search_kwargs={'k':5})\n",
    "\n",
    "# Ensemble Retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[chroma_retriever, bm25_retriever],\n",
    "    weights=[0.5, 0.5]        \n",
    ")\n",
    "\n",
    "# Create MultiQueryRetriever\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=ensemble_retriever,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "def _get_context_retriever_chain(vector_db, ensemble_retriever, llm):\n",
    "    multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "        retriever=ensemble_retriever,\n",
    "        llm=llm\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation, focusing on the most recent messages.\"),\n",
    "    ])\n",
    "    retriever_chain = create_history_aware_retriever(llm, ensemble_retriever, prompt)\n",
    "\n",
    "    return retriever_chain\n",
    "\n",
    "def get_conversational_rag_chain(llm, retriever):\n",
    "    retriever_chain = _get_context_retriever_chain(vector_db, retriever, llm)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "        \"\"\"You are an assistant designed specifically for answering queries based on company regulations. Always respond strictly according to the company's internal regulations, ensuring your answers are aligned with these rules. \n",
    "        When providing an answer, first cite the most relevant regulation in detail, including chapter and section numbers if applicable. If multiple regulations apply, list all relevant ones before giving your response. \n",
    "        Your goal is to provide the user with clear guidance based on the regulations, so be as specific as possible with the details of the rules and regulations before proceeding with the final answer.\n",
    "        If no regulation directly applies, inform the user and give your best guidance based on your knowledge of the company's practices.\n",
    "        \n",
    "        After your explanation, provide the exact quotes from the relevant regulations under a \"Source Regulations:\" section. Format each quote as follows:\n",
    "        [Document Name] Chapter X, Section Y: \"Exact quote from the regulation\"\n",
    "\n",
    "        {context}\"\"\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "    ])\n",
    "    stuff_documents_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "    return create_retrieval_chain(retriever_chain, stuff_documents_chain)\n",
    "\n",
    "llm_stream_openai = ChatOpenAI(\n",
    "    model=\"gpt-4\",  # Here you could use \"gpt-4-1106-preview\" or \"gpt-4-0125-preview\" if you have access to them\n",
    "    temperature=0,\n",
    "    streaming=True,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi there! How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"일이 너무 많아서 연차휴가를 할당된 만큼 다 쓰지 못할거 같아. 그럼 남는 연차휴가는 어떻게 되지? 남는 연차 휴가에 대해 돈으로 받을수 있어?\"},\n",
    "]\n",
    "messages = [HumanMessage(content=m[\"content\"]) if m[\"role\"] == \"user\" else AIMessage(content=m[\"content\"]) for m in messages]\n",
    "\n",
    "conversation_rag_chain = get_conversational_rag_chain(llm_stream_openai, multi_query_retriever)\n",
    "response_message = \"*(RAG Response)*\\n\"\n",
    "for chunk in conversation_rag_chain.pick(\"answer\").stream({\"messages\": messages[:-1], \"input\": messages[-1].content}):\n",
    "    response_message += chunk\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "messages.append({\"role\": \"assistant\", \"content\": response_message})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kongasAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
